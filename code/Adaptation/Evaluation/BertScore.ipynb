{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4412d3d",
   "metadata": {},
   "source": [
    "# Inter-lingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1995516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: sagorsarker/bangla-bert-base\n",
      "Full Sentence BERTScore (F1): 0.8898\n",
      "Individual CSI BERTScore (F1): [0.39224880933761597, 1.0]\n",
      "Aggregate CSI BERTScore (F1): 0.6961\n"
     ]
    }
   ],
   "source": [
    "from bert_score import BERTScorer\n",
    "from transformers import AutoModel\n",
    "import re\n",
    "\n",
    "# Specify the model name\n",
    "model_name = 'sagorsarker/bangla-bert-base' # 'ai4bharat/indic-bert' # 'neuropark/sahajBERT'\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the scorer with the model and language settings\n",
    "scorer = BERTScorer(model_type=model_name, lang=\"bn\", num_layers=model.config.num_hidden_layers)\n",
    "\n",
    "# Example input texts with CSI tags\n",
    "ground_truth = \"নির্বাহী তার <CSI>সহকর্মীকে</CSI> অভিনন্দন জানাতে দৃঢ়ভাবে <CSI>হ্যান্ডশেক</CSI> করলেন।\"\n",
    "output = \"আমেরিকান নির্বাহী তার সহকর্মীকে <CSI>অভিনন্দন</CSI> জানাতে একটি দৃঢ় <CSI>হ্যান্ডশেক</CSI> দিলেন\"\n",
    "\n",
    "# Function to extract CSI-tagged phrases\n",
    "def extract_all_csi(text):\n",
    "    return re.findall(r'<CSI>(.*?)</CSI>', text)\n",
    "\n",
    "# Extract CSI segments\n",
    "gt_csi_list = extract_all_csi(ground_truth)\n",
    "out_csi_list = extract_all_csi(output)\n",
    "\n",
    "# Ensure equal number of CSI tags\n",
    "assert len(gt_csi_list) == len(out_csi_list), \"Mismatched CSI tags in ground truth and output!\"\n",
    "\n",
    "# Compute BERTScore for full sentences\n",
    "P_full, R_full, F1_full = scorer.score([output], [ground_truth])\n",
    "full_f1 = F1_full.mean().item()\n",
    "\n",
    "# Compute BERTScore for each CSI segment\n",
    "csi_scores = []\n",
    "for gt_csi, out_csi in zip(gt_csi_list, out_csi_list):\n",
    "    P, R, F1 = scorer.score([out_csi], [gt_csi])\n",
    "    csi_scores.append(F1.mean().item())\n",
    "\n",
    "# Calculate average CSI score\n",
    "avg_csi_score = sum(csi_scores) / len(csi_scores) if csi_scores else 0.0\n",
    "\n",
    "# Print results\n",
    "print(f\"Using model: {model_name}\")\n",
    "print(f\"Full Sentence BERTScore (F1): {full_f1:.4f}\")\n",
    "print(f\"Individual CSI BERTScore (F1): {csi_scores}\")\n",
    "print(f\"Aggregate CSI BERTScore (F1): {avg_csi_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa3a540",
   "metadata": {},
   "source": [
    "# Intra-lingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e41ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Sentence BERTScore (F1): 0.8200\n",
      "Individual CSI BERTScore (F1): [0.2082308977842331]\n",
      "Aggregate CSI BERTScore (F1): 0.2082\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "import re\n",
    "\n",
    "# Example with multiple CSI tags\n",
    "ground_truth = \"The executive gave a firm <CSI>handshake</CSI> to congratulate his colleague.\"\n",
    "output = \"The American executive gave a firm <CSI> appreciative shoulder tap </CSI> to congratulate his colleague.\"\n",
    "\n",
    "# Extract ALL CSI-tagged phrases\n",
    "def extract_all_csi(text):\n",
    "    return re.findall(r'<CSI>(.*?)</CSI>', text)\n",
    "\n",
    "gt_csi_list = extract_all_csi(ground_truth)\n",
    "out_csi_list = extract_all_csi(output)\n",
    "\n",
    "# Ensure equal number of CSI tags\n",
    "assert len(gt_csi_list) == len(out_csi_list), \"Mismatched CSI tags in ground truth and output!\"\n",
    "\n",
    "# Compute BERTScore for full sentences\n",
    "P_full, R_full, F1_full = score([output], [ground_truth], lang=\"en\", model_type= 'bert-base-uncased', rescale_with_baseline=True)\n",
    "\n",
    "# Compute BERTScore for each CSI segment\n",
    "csi_scores = []\n",
    "for gt_csi, out_csi in zip(gt_csi_list, out_csi_list):\n",
    "    P, R, F1 = score([out_csi], [gt_csi], lang=\"en\", model_type= 'bert-base-uncased', rescale_with_baseline=True)\n",
    "    csi_scores.append(F1.mean().item())\n",
    "\n",
    "# Aggregate CSI scores (average)\n",
    "avg_csi_score = sum(csi_scores) / len(csi_scores) if csi_scores else 0.0\n",
    "\n",
    "# Print results\n",
    "print(f\"Full Sentence BERTScore (F1): {F1_full.mean().item():.4f}\")\n",
    "print(f\"Individual CSI BERTScore (F1): {csi_scores}\")\n",
    "print(f\"Aggregate CSI BERTScore (F1): {avg_csi_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d364f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "adaptation_df = pd.read_csv(\"../../../Datasets/Adaptation_Final.csv\")\n",
    "gemini_df = pd.read_csv(\"../../../Output/Adaptation/gemini.csv\")\n",
    "qwen_df = pd.read_csv(\"../../../Output/Adaptation/qwen.csv\")\n",
    "claude_df = pd.read_csv(\"../../../Output/Adaptation/claude.csv\")\n",
    "llama_df = pd.read_csv(\"../../../Output/Adaptation/llama.csv\")\n",
    "deepseek_df = pd.read_csv(\"../../../Output/Adaptation/deepseek.csv\")\n",
    "\n",
    "working_df = qwen_df.copy()\n",
    "# adapt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa5070d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 71/524 [02:24<15:21,  2.03s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 97\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Intra Hindu\u001b[39;00m\n\u001b[1;32m     96\u001b[0m gt_intra_hindu \u001b[38;5;241m=\u001b[39m adaptation_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIntra-lingual (Hindu)\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\n\u001b[0;32m---> 97\u001b[0m full_f1, _, avg_csi \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_intra_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_intra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_intra_hindu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_f1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     intra_hindu_full_f1\u001b[38;5;241m.\u001b[39mappend(full_f1)\n",
      "Cell \u001b[0;32mIn[5], line 48\u001b[0m, in \u001b[0;36mcompute_intra_scores\u001b[0;34m(output, ground_truth)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Full sentence score\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m P_full, R_full, F1_full \u001b[38;5;241m=\u001b[39m \u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrescale_with_baseline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m full_f1 \u001b[38;5;241m=\u001b[39m F1_full\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# CSI segment scores\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/bert_score/score.py:123\u001b[0m, in \u001b[0;36mscore\u001b[0;34m(cands, refs, model_type, num_layers, verbose, idf, device, batch_size, nthreads, all_layers, lang, return_hash, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalculating scores...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 123\u001b[0m all_preds \u001b[38;5;241m=\u001b[39m \u001b[43mbert_cos_score_idf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43midf_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ref_group_boundaries \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     max_preds \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/bert_score/utils.py:607\u001b[0m, in \u001b[0;36mbert_cos_score_idf\u001b[0;34m(model, refs, hyps, tokenizer, idf_dict, verbose, batch_size, device, all_layers)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdedup_and_sort\u001b[39m(l):\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(l)), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 607\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43mdedup_and_sort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhyps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m embs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    609\u001b[0m iter_range \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/bert_score/utils.py:605\u001b[0m, in \u001b[0;36mbert_cos_score_idf.<locals>.dedup_and_sort\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdedup_and_sort\u001b[39m(l):\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.9/site-packages/bert_score/utils.py:605\u001b[0m, in \u001b[0;36mbert_cos_score_idf.<locals>.dedup_and_sort.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdedup_and_sort\u001b[39m(l):\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(l)), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bert_score import BERTScorer, score\n",
    "from transformers import AutoModel\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "adaptation_df = pd.read_csv(\"../../../Datasets/Adaptation_Final.csv\")\n",
    "# gemini_df = pd.read_csv(\"../../../Output/Adaptation/gemini.csv\")\n",
    "qwen_df = pd.read_csv(\"../../../Output/Adaptation/qwen.csv\")\n",
    "# claude_df = pd.read_csv(\"../../../Output/Adaptation/claude.csv\")\n",
    "# llama_df = pd.read_csv(\"../../../Output/Adaptation/llama.csv\")\n",
    "# deepseek_df = pd.read_csv(\"../../../Output/Adaptation/deepseek.csv\")\n",
    "\n",
    "model_name = 'sagorsarker/bangla-bert-base'\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "scorer_bn = BERTScorer(model_type=model_name, lang=\"bn\", num_layers=model.config.num_hidden_layers)\n",
    "\n",
    "# def extract_all_csi(text):\n",
    "#     return re.findall(r'<CSI>(.*?)</CSI>', text)\n",
    "\n",
    "def extract_all_csi(text):\n",
    "    \"\"\"\n",
    "    Extract all content between <CSI> tags from the given text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input string or any other type\n",
    "        \n",
    "    Returns:\n",
    "        list: List of found CSI items (empty list if none found or invalid input)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        return re.findall(r'<CSI>(.*?)</CSI>', text)\n",
    "    except (TypeError, re.error):\n",
    "        return []\n",
    "\n",
    "def compute_intra_scores(output, ground_truth):\n",
    "    gt_csi_list = extract_all_csi(ground_truth)\n",
    "    out_csi_list = extract_all_csi(output)\n",
    "    \n",
    "    if len(gt_csi_list) != len(out_csi_list):\n",
    "        return None, None, None\n",
    "    \n",
    "    # Full sentence score\n",
    "    P_full, R_full, F1_full = score([output], [ground_truth], lang=\"en\", model_type='bert-base-uncased', rescale_with_baseline=True)\n",
    "    full_f1 = F1_full.mean().item()\n",
    "    \n",
    "    # CSI segment scores\n",
    "    csi_scores = []\n",
    "    for gt_csi, out_csi in zip(gt_csi_list, out_csi_list):\n",
    "        P, R, F1 = score([out_csi], [gt_csi], lang=\"en\", model_type='bert-base-uncased', rescale_with_baseline=True)\n",
    "        csi_scores.append(F1.mean().item())\n",
    "    \n",
    "    avg_csi_score = sum(csi_scores) / len(csi_scores) if csi_scores else 0.0\n",
    "    return full_f1, csi_scores, avg_csi_score\n",
    "\n",
    "def compute_inter_scores(output, ground_truth, scorer):\n",
    "    gt_csi_list = extract_all_csi(ground_truth)\n",
    "    out_csi_list = extract_all_csi(output)\n",
    "    \n",
    "    if len(gt_csi_list) != len(out_csi_list):\n",
    "        return None, None, None\n",
    "    \n",
    "    # Full sentence score\n",
    "    P_full, R_full, F1_full = scorer.score([output], [ground_truth])\n",
    "    full_f1 = F1_full.mean().item()\n",
    "    \n",
    "    # CSI segment scores\n",
    "    csi_scores = []\n",
    "    for gt_csi, out_csi in zip(gt_csi_list, out_csi_list):\n",
    "        P, R, F1 = scorer.score([out_csi], [gt_csi])\n",
    "        csi_scores.append(F1.mean().item())\n",
    "    \n",
    "    avg_csi_score = sum(csi_scores) / len(csi_scores) if csi_scores else 0.0\n",
    "    return full_f1, csi_scores, avg_csi_score\n",
    "\n",
    "# Lists to collect scores\n",
    "intra_hindu_full_f1 = []\n",
    "intra_hindu_avg_csi = []\n",
    "intra_muslim_full_f1 = []\n",
    "intra_muslim_avg_csi = []\n",
    "inter_hindu_full_f1 = []\n",
    "inter_hindu_avg_csi = []\n",
    "inter_muslim_full_f1 = []\n",
    "inter_muslim_avg_csi = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(working_df))):\n",
    "    # Intra-lingua evaluations\n",
    "    output_intra = working_df['Intra'][i]\n",
    "    \n",
    "    # Intra Hindu\n",
    "    gt_intra_hindu = adaptation_df['Intra-lingual (Hindu)'][i]\n",
    "    full_f1, _, avg_csi = compute_intra_scores(output_intra, gt_intra_hindu)\n",
    "    if full_f1 is not None:\n",
    "        intra_hindu_full_f1.append(full_f1)\n",
    "        intra_hindu_avg_csi.append(avg_csi)\n",
    "    \n",
    "    # Intra Muslim\n",
    "    gt_intra_muslim = adaptation_df['Intra-lingual (Muslim)'][i]\n",
    "    full_f1, _, avg_csi = compute_intra_scores(output_intra, gt_intra_muslim)\n",
    "    if full_f1 is not None:\n",
    "        intra_muslim_full_f1.append(full_f1)\n",
    "        intra_muslim_avg_csi.append(avg_csi)\n",
    "    \n",
    "    # Inter-lingua evaluations\n",
    "    output_inter = working_df['Inter'][i]\n",
    "    \n",
    "    # Inter Hindu\n",
    "    gt_inter_hindu = adaptation_df['Inter-lingual (Hindu)'][i]\n",
    "    full_f1, _, avg_csi = compute_inter_scores(output_inter, gt_inter_hindu, scorer_bn)\n",
    "    if full_f1 is not None:\n",
    "        inter_hindu_full_f1.append(full_f1)\n",
    "        inter_hindu_avg_csi.append(avg_csi)\n",
    "    \n",
    "    # Inter Muslim\n",
    "    gt_inter_muslim = adaptation_df['Inter-lingual (Muslim)'][i]\n",
    "    full_f1, _, avg_csi = compute_inter_scores(output_inter, gt_inter_muslim, scorer_bn)\n",
    "    if full_f1 is not None:\n",
    "        inter_muslim_full_f1.append(full_f1)\n",
    "        inter_muslim_avg_csi.append(avg_csi)\n",
    "\n",
    "results = {\n",
    "    'Adaptation': ['Intra-lingual (Hindu)', 'Intra-lingual (Muslim)', 'Inter-lingual (Hindu)', 'Inter-lingual (Muslim)'],\n",
    "    'Avg Full Sentence F1': [\n",
    "        np.mean(intra_hindu_full_f1) if intra_hindu_full_f1 else 0.0,\n",
    "        np.mean(intra_muslim_full_f1) if intra_muslim_full_f1 else 0.0,\n",
    "        np.mean(inter_hindu_full_f1) if inter_hindu_full_f1 else 0.0,\n",
    "        np.mean(inter_muslim_full_f1) if inter_muslim_full_f1 else 0.0\n",
    "    ],\n",
    "    'Avg Aggregate CSI F1': [\n",
    "        np.mean(intra_hindu_avg_csi) if intra_hindu_avg_csi else 0.0,\n",
    "        np.mean(intra_muslim_avg_csi) if intra_muslim_avg_csi else 0.0,\n",
    "        np.mean(inter_hindu_avg_csi) if inter_hindu_avg_csi else 0.0,\n",
    "        np.mean(inter_muslim_avg_csi) if inter_muslim_avg_csi else 0.0\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f971c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
