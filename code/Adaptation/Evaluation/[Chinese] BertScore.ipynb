{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4412d3d",
   "metadata": {},
   "source": [
    "# Inter-lingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1995516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: bert-base-chinese\n",
      "Full Sentence BERTScore (F1): 0.6788\n",
      "Individual CSI BERTScore (F1): [0.6494137644767761]\n",
      "Aggregate CSI BERTScore (F1): 0.6494\n"
     ]
    }
   ],
   "source": [
    "from bert_score import BERTScorer\n",
    "from transformers import AutoModel\n",
    "import re\n",
    "\n",
    "# Specify the model name\n",
    "model_name = 'bert-base-chinese'\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the scorer with the model and language settings\n",
    "scorer = BERTScorer(model_type=model_name, lang=\"zh\", num_layers=model.config.num_hidden_layers)\n",
    "\n",
    "# Example input texts with CSI tags\n",
    "ground_truth = \"一起喝杯<CSI>茶</CSI>，聊聊近况吧！\"\n",
    "output = \"美国商务人士与中国合作伙伴进行了一次愉快的<CSI>会谈</CSI>\"\n",
    "\n",
    "# Function to extract CSI-tagged phrases\n",
    "def extract_all_csi(text):\n",
    "    return re.findall(r'<CSI>(.*?)</CSI>', text)\n",
    "\n",
    "# Extract CSI segments\n",
    "gt_csi_list = extract_all_csi(ground_truth)\n",
    "out_csi_list = extract_all_csi(output)\n",
    "\n",
    "# Ensure equal number of CSI tags\n",
    "assert len(gt_csi_list) == len(out_csi_list), \"Mismatched CSI tags in ground truth and output!\"\n",
    "\n",
    "# Compute BERTScore for full sentences\n",
    "P_full, R_full, F1_full = scorer.score([output], [ground_truth])\n",
    "full_f1 = F1_full.mean().item()\n",
    "\n",
    "# Compute BERTScore for each CSI segment\n",
    "csi_scores = []\n",
    "for gt_csi, out_csi in zip(gt_csi_list, out_csi_list):\n",
    "    P, R, F1 = scorer.score([out_csi], [gt_csi])\n",
    "    csi_scores.append(F1.mean().item())\n",
    "\n",
    "# Calculate average CSI score\n",
    "avg_csi_score = sum(csi_scores) / len(csi_scores) if csi_scores else 0.0\n",
    "\n",
    "# Print results\n",
    "print(f\"Using model: {model_name}\")\n",
    "print(f\"Full Sentence BERTScore (F1): {full_f1:.4f}\")\n",
    "print(f\"Individual CSI BERTScore (F1): {csi_scores}\")\n",
    "print(f\"Aggregate CSI BERTScore (F1): {avg_csi_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa3a540",
   "metadata": {},
   "source": [
    "# Intra-lingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e41ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "import re\n",
    "\n",
    "# Example with multiple CSI tags\n",
    "ground_truth = \"The executive gave a firm <CSI>handshake</CSI> to congratulate his colleague.\"\n",
    "output = \"The American executive gave a firm <CSI> appreciative shoulder tap </CSI> to congratulate his colleague.\"\n",
    "\n",
    "# Extract ALL CSI-tagged phrases\n",
    "def extract_all_csi(text):\n",
    "    return re.findall(r'<CSI>(.*?)</CSI>', text)\n",
    "\n",
    "gt_csi_list = extract_all_csi(ground_truth)\n",
    "out_csi_list = extract_all_csi(output)\n",
    "\n",
    "# Ensure equal number of CSI tags\n",
    "assert len(gt_csi_list) == len(out_csi_list), \"Mismatched CSI tags in ground truth and output!\"\n",
    "\n",
    "# Compute BERTScore for full sentences\n",
    "P_full, R_full, F1_full = score([output], [ground_truth], lang=\"en\", model_type= 'bert-base-uncased', rescale_with_baseline=True)\n",
    "\n",
    "# Compute BERTScore for each CSI segment\n",
    "csi_scores = []\n",
    "for gt_csi, out_csi in zip(gt_csi_list, out_csi_list):\n",
    "    P, R, F1 = score([out_csi], [gt_csi], lang=\"en\", model_type= 'bert-base-uncased', rescale_with_baseline=True)\n",
    "    csi_scores.append(F1.mean().item())\n",
    "\n",
    "# Aggregate CSI scores (average)\n",
    "avg_csi_score = sum(csi_scores) / len(csi_scores) if csi_scores else 0.0\n",
    "\n",
    "# Print results\n",
    "print(f\"Full Sentence BERTScore (F1): {F1_full.mean().item():.4f}\")\n",
    "print(f\"Individual CSI BERTScore (F1): {csi_scores}\")\n",
    "print(f\"Aggregate CSI BERTScore (F1): {avg_csi_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d364f9",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\nadaptation_df = pd.read_csv(\"../../../Datasets/Adaptation_Final.csv\")\ngemini_df = pd.read_csv(\"../../../Output/Adaptation/gemini.csv\")\nadaptation_df.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5070d4",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nfrom bert_score import BERTScorer, score\nfrom transformers import AutoModel\nimport re\nimport numpy as np\nfrom tqdm import tqdm\n\nadaptation_df = pd.read_csv(\"../../../Datasets/Adaptation_Final.csv\")\ngemini_df = pd.read_csv(\"../../../Output/Adaptation/gpt.csv\")\n\nmodel_name = 'bert-base-chinese'\nmodel = AutoModel.from_pretrained(model_name)\nscorer_zh = BERTScorer(model_type=model_name, lang=\"zh\", num_layers=model.config.num_hidden_layers)\n\ndef extract_all_csi(text):\n    \"\"\"\n    Extract all content between <CSI> tags from the given text.\n    \n    Args:\n        text: Input string or any other type\n        \n    Returns:\n        list: List of found CSI items (empty list if none found or invalid input)\n    \"\"\"\n    if not isinstance(text, str) or not text.strip():\n        return []\n    \n    try:\n        return re.findall(r'<CSI>(.*?)</CSI>', text)\n    except (TypeError, re.error):\n        return []\n\ndef compute_intra_scores(output, ground_truth):\n    gt_csi_list = extract_all_csi(ground_truth)\n    out_csi_list = extract_all_csi(output)\n    \n    if len(gt_csi_list) != len(out_csi_list):\n        return None, None, None\n    \n    # Full sentence score\n    P_full, R_full, F1_full = score([output], [ground_truth], lang=\"en\", model_type='bert-base-uncased', rescale_with_baseline=True)\n    full_f1 = F1_full.mean().item()\n    \n    # CSI segment scores\n    csi_scores = []\n    for gt_csi, out_csi in zip(gt_csi_list, out_csi_list):\n        P, R, F1 = score([out_csi], [gt_csi], lang=\"en\", model_type='bert-base-uncased', rescale_with_baseline=True)\n        csi_scores.append(F1.mean().item())\n    \n    avg_csi_score = sum(csi_scores) / len(csi_scores) if csi_scores else 0.0\n    return full_f1, csi_scores, avg_csi_score\n\ndef compute_inter_scores(output, ground_truth, scorer):\n    gt_csi_list = extract_all_csi(ground_truth)\n    out_csi_list = extract_all_csi(output)\n    \n    if len(gt_csi_list) != len(out_csi_list):\n        return None, None, None\n    \n    # Full sentence score\n    P_full, R_full, F1_full = scorer.score([output], [ground_truth])\n    full_f1 = F1_full.mean().item()\n    \n    # CSI segment scores\n    csi_scores = []\n    for gt_csi, out_csi in zip(gt_csi_list, out_csi_list):\n        P, R, F1 = scorer.score([out_csi], [gt_csi])\n        csi_scores.append(F1.mean().item())\n    \n    avg_csi_score = sum(csi_scores) / len(csi_scores) if csi_scores else 0.0\n    return full_f1, csi_scores, avg_csi_score\n\n# Lists to collect scores\nintra_hindu_full_f1 = []\nintra_hindu_avg_csi = []\nintra_muslim_full_f1 = []\nintra_muslim_avg_csi = []\ninter_hindu_full_f1 = []\ninter_hindu_avg_csi = []\ninter_muslim_full_f1 = []\ninter_muslim_avg_csi = []\n\n\nfor i in tqdm(range(len(gemini_df))):\n    # Intra-lingua evaluations\n    output_intra = gemini_df['Intra'][i]\n    \n    # Intra Hindu\n    gt_intra_hindu = adaptation_df['Intra-lingual (Hindu)'][i]\n    full_f1, _, avg_csi = compute_intra_scores(output_intra, gt_intra_hindu)\n    if full_f1 is not None:\n        intra_hindu_full_f1.append(full_f1)\n        intra_hindu_avg_csi.append(avg_csi)\n    \n    # Intra Muslim\n    gt_intra_muslim = adaptation_df['Intra-lingual (Muslim)'][i]\n    full_f1, _, avg_csi = compute_intra_scores(output_intra, gt_intra_muslim)\n    if full_f1 is not None:\n        intra_muslim_full_f1.append(full_f1)\n        intra_muslim_avg_csi.append(avg_csi)\n    \n    # Inter-lingua evaluations\n    output_inter = gemini_df['Inter'][i]\n    \n    # Inter Hindu\n    gt_inter_hindu = adaptation_df['Inter-lingual (Hindu)'][i]\n    full_f1, _, avg_csi = compute_inter_scores(output_inter, gt_inter_hindu, scorer_zh)\n    if full_f1 is not None:\n        inter_hindu_full_f1.append(full_f1)\n        inter_hindu_avg_csi.append(avg_csi)\n    \n    # Inter Muslim\n    gt_inter_muslim = adaptation_df['Inter-lingual (Muslim)'][i]\n    full_f1, _, avg_csi = compute_inter_scores(output_inter, gt_inter_muslim, scorer_zh)\n    if full_f1 is not None:\n        inter_muslim_full_f1.append(full_f1)\n        inter_muslim_avg_csi.append(avg_csi)\n\nresults = {\n    'Adaptation': ['Intra-lingual (Hindu)', 'Intra-lingual (Muslim)', 'Inter-lingual (Hindu)', 'Inter-lingual (Muslim)'],\n    'Avg Full Sentence F1': [\n        np.mean(intra_hindu_full_f1) if intra_hindu_full_f1 else 0.0,\n        np.mean(intra_muslim_full_f1) if intra_muslim_full_f1 else 0.0,\n        np.mean(inter_hindu_full_f1) if inter_hindu_full_f1 else 0.0,\n        np.mean(inter_muslim_full_f1) if inter_muslim_full_f1 else 0.0\n    ],\n    'Avg Aggregate CSI F1': [\n        np.mean(intra_hindu_avg_csi) if intra_hindu_avg_csi else 0.0,\n        np.mean(intra_muslim_avg_csi) if intra_muslim_avg_csi else 0.0,\n        np.mean(inter_hindu_avg_csi) if inter_hindu_avg_csi else 0.0,\n        np.mean(inter_muslim_avg_csi) if inter_muslim_avg_csi else 0.0\n    ]\n}\n\nresults_df = pd.DataFrame(results)\nprint(results_df)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f971c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}